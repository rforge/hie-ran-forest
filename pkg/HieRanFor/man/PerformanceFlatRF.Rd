% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/PerformanceFlatRF.R
\name{PerformanceFlatRF}
\alias{PerformanceFlatRF}
\title{Runs a flat classification on the same data as \code{hie.RF} and assess
performance.}
\usage{
PerformanceFlatRF(hie.RF, mtry = hie.RF$call$mtry,
  ntree = hie.RF$call$ntree, importance = hie.RF$call$importance,
  proximity = hie.RF$call$proximity, keep.forest = hie.RF$call$keep.forest,
  keep.inbag = hie.RF$call$keep.inbag, per.index = c("flat.measures",
  "hie.F.measure"), crisp.rule = c("multiplicative.majority",
  "multiplicative.permutation"), perm.num = 500, by.node = TRUE,
  div.logical = TRUE, div.print = 25, beta.h.F = 1, ...)
}
\arguments{
\item{hie.RF}{Object of class \code{"HRF"} - the output of
  \code{RunHRF}.}

\item{mtry}{Number of variables randomly sampled as candidates
  at each split. Default is to use the same method used in \code{hie.RF}.}

\item{ntree}{Number of trees to grow in the flat classifier. See
  \code{\link{randomForest}} for additional details. Default is the same
  \code{ntree} used in each local classifier of \code{hie.RF}.}

\item{importance}{Logical, if \code{TRUE}, importance of variables
  will be assessed. See \code{\link{randomForest}} for additional details.
  Default is the same as \code{hie.RF}.}

\item{proximity}{Logical, If \code{TRUE}, proximity will be
  calculated. See \code{\link{randomForest}} for additional details. Default
  is the same as \code{hie.RF}.}

\item{keep.forest}{Logical, if \code{TRUE} (recommended) the forest
  will be retained. Default is the same as \code{hie.RF}.}

\item{keep.inbag}{Logical, if \code{TRUE} an \emph{n} by \code{ntree}
  matrix be returned that keeps track of which cases are out-of-bag in which
  trees. \emph{n} being the number of cases in the training set. Required for
  votes extraction. Default is the same as \code{hie.RF}.}

\item{per.index}{The performance and accuracy indices to compute.
  See details in \code{\link{PerformanceHRF}}.}

\item{crisp.rule}{The method of selecting a single crisp class from
  the proportion of votes. See details in \code{\link{PerformanceHRF}}.}

\item{perm.num}{Integer, number of random permutations for each
  case if \code{'multiplicative.permutation'} is applied. See details in
  \code{\link{PerformanceHRF}}.}

\item{by.node}{Logical, if \code{TRUE}, performance measures are
  estimated for each terminal node as well.}

\item{div.logical}{Logical, if \code{TRUE} progress when
  \code{'multiplicative.permutation'} is applied will be printed every
  \code{div.print} permutations.}

\item{div.print}{See above.}

\item{beta.h.F}{Numeric in the range \code{beta.h.F} => 0. Controls
  weights in the hierarchical F measure index. See \code{\link{HieFMeasure}}
  for details.}

\item{\dots}{Optional parameters to be passed to low level functions.}
}
\value{
List with the following components:
 {\tabular{lll}{
  \code{'flat.RF'}                \tab \tab An object of class
  \code{randomForest} for the flat classification  \cr

  \code{'crisp.case.class'}       \tab \tab Data frame containing the crisp
  class for each case based on all options defined by \code{crisp.rule}. The
  observed class (terminal node) is given at the last column under
  \code{obs.term.node}. \cr

  \code{'hie.performance'}        \tab \tab Data frame summarizing all the
  performance measures, starting with the overall performance indices and
  followed by all the \code{by.node} measures. See details in
  \code{\link{PerformanceHRF}}. \cr

  \code{'nodes.measures.columns'} \tab \tab Optional, if \code{by.node=TRUE},
  data frame with three columns including the name of the terminal node, the
  performance index and the name of the column in \code{hie.performance} that
  holds the output. \cr

  \code{'call'}                   \tab \tab The call to function
  \code{PerformanceFlatHRF}. \cr}}
}
\description{
This function takes as input an object of class \code{HRF}, identifies the
observed terminal node for each case of the original training data, and runs
a regular flat classification on all terminal nodes. The function then
applies two different methods for selecting a single terminal node (given by
\code{crisp.rule} and described in \code{\link{PerformanceHRF}}). Finally,
the performance is explored in relation to the observed class using various
perfromance measures (given by \code{per.index})
}
\details{
When running a flat classification, a single randomForest algorithm
is used to distinguish between all terminal nodes. As no information on the
proportion of votes along the class hierarchy is produced, the
\code{"stepwise.majority"} option for \code{crisp.rule} cannot be
calculated.\cr
This function allows comparison of the performance of the hierarchical and
flat randomForest. In addition, the proportion of votes produced by this
function (can be extracted from the\code{randomForest} object) is
comparable to the proportion of votes returned by
\code{\link{PerformanceHRF}} in the \code{'multiplicative.prop'} data
frame.
}
\examples{
# analyse the OliveOilHie dataset
data(OliveOilHie)
hie.RF.OO <- RunHRF(train.data        = OliveOilHie,
                    case.ID           = "case.ID",
                    hie.levels        = c(2:4),
                    mtry              = "tuneRF2",
                    internal.end.path = TRUE)

# run and assess performance as flat classification
flat.RF.OO <- PerformanceFlatRF(hie.RF     = hie.RF.OO,
                                per.index  = c("flat.measures",
                                               "hie.F.measure"),
                                crisp.rule = c("multiplicative.majority",
                                               "multiplicative.permutation"),
                                perm.num   = 10,         #
                                div.print  = 2)
# extract values
names(flat.RF.OO)
flat.RF.OO.RF        <- flat.RF.OO$flat.RF # object of class randomForest
votes.flat           <- flat.RF.OO$flat.RF$votes
flat.RF.OO.crisp     <- flat.RF.OO$crisp.case.class
hie.perf.flat        <- flat.RF.OO$hie.performance
flat.RF.OO.nodes.mea <- flat.RF.OO$nodes.measures.columns
flat.RF.OO.call      <- flat.RF.OO$call

# compare the hie.perf.flat to the HRF analysis
# Performance of the hierarchical randomForest
perf.hRF.OO <- PerformanceHRF(hie.RF     = hie.RF.OO,
                              crisp.rule  = c("multiplicative.majority",
                                              "multiplicative.permutation"),
                              perm.num    = 10,
                              div.print   = 2)

hie.perf.HRF   <- perf.hRF.OO$hie.performance

# Despite the overall high performance, HRF is slightly better...
comp.perf <- data.frame(model = c("Flat","HRF"))
join.perf <- rbind(hie.perf.flat[1, c("Accuracy","Kappa","h.F.measure")],
                   hie.perf.HRF[1, c("Accuracy","Kappa","h.F.measure")])
comp.perf <- cbind(data.frame(model = c("Flat","HRF")),
                   join.perf)
comp.perf
}
\author{
Yoni Gavish <gavishyoni@gmail.com>
}
\seealso{
\code{\link{RunHRF}} for running a hierarchical randomForest analysis,
\code{\link{PerformanceHRF}} for performance analysis,
\code{\link{HieFMeasure}} for additional information on the hierarchical
performance measures.
}

