% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/PerformanceHRF.R
\name{PerformanceHRF}
\alias{PerformanceHRF}
\title{Flat and hierarchical performance measures.}
\usage{
PerformanceHRF(hie.RF, per.index = c("flat.measures", "hie.F.measure"),
  crisp.rule = c("stepwise.majority", "multiplicative.majority",
  "multiplicative.permutation"), perm.num = 500, by.node = TRUE,
  div.logical = TRUE, div.print = 25, beta.h.F = 1, ...)
}
\arguments{
\item{hie.RF}{Object of class \code{"HRF"} - the output of
  \code{RunHRF}.}

\item{per.index}{The performance and accuracy indices to compute. See
  details below.}

\item{crisp.rule}{The method of selecting a single crisp class from
  the proportion of votes. See details below.}

\item{perm.num}{Integer, number of random permutations for each case
  if \code{'multiplicative.permutation'} is applied.}

\item{by.node}{Logical, if \code{TRUE}, performances indices will
  be estimated for each terminal node as well as for the overall confusion
  matrix.}

\item{div.logical}{Logical, if \code{TRUE} progress when
  \code{'multiplicative.permutation'} is applied will be printed every
  \code{div.print} permutations.}

\item{div.print}{See above.}

\item{beta.h.F}{Numeric in the range \code{beta.h.F} >= 0. Controls
  weights in the hierarchical F measure index. See \code{\link{HieFMeasure}}
  for details.}

\item{\dots}{Optional parameters to be passed to low level functions.}
}
\value{
A list with the following components:

{\tabular{lll}{
\code{"raw.vote"}               \tab \tab Data frame containing for each
case, the proportion of votes for each node in each local classifier (the
output of \code{\link{predict.HRF}}). \cr

\code{"crisp.case.class"}       \tab \tab Data frame containing the crisp
class for each case based on all options defined by \code{crisp.rule}. The
observed class (terminal node) is given at the last column under
\code{obs.term.node}. \cr

\code{"hie.performance"}        \tab \tab Data frame summarizing all the
performance measures, starting with the overall performance indices and
followed by all the \code{by.node} measures. See details below. \cr

\code{"multiplicative.prop"}    \tab \tab Optional data frame with the
multiplicative proportion of votes (the output of
\code{\link{GetMultPropVotes}}). Will be returned if
\code{"multiplicative.majority"} or \code{"multiplicative.permutation"} were
selected.\cr

\code{"nodes.measures.columns"} \tab \tab Optional, if \code{by.node=TRUE},
data frame with three columns including the name of the terminal node, the
performance index and the name of the column in \code{hie.performance} that
holds the output. \cr

\code{"call"}                   \tab \tab The call to function
\code{PerformanceHRF}. \cr}}
}
\description{
This function predicts the proportion of votes for each case in each local
classifier and then estimates various flat and hierarchical performance
measures. The performance measures are based on translating the proportion of
votes to a crisp class (selecting a single class). The function offers three
different method for translating the soft proportion of votes to a crisp
class.
}
\details{
For a given flat classification, the output of the randomForest
  algorithm for each case is the proportion of out-of-bag (OOB) votes that
  each class (node) received. In the hierarchical version of randomForest,
  the proportion of OOB votes are returned separately for each local
  classifier (see \code{\link{predict.HRF}}). In flat classification, the
  proportions of votes are translated into a crisp classification by applying
  the majority rule, i.e., by selecting the class with the highest proportion
  of votes. In the hierarchical version of randomForest, there are 3
  different methods to select one crisp class for a given case. \cr

\strong{The \code{"stepwise.majority"} method: } \cr In each local
classifier, the flat majority rule is applied. Then starting with the
\code{tree.root}, a case is classified down the tree until a terminal node is
reached. This method gives high emphasis to local classifiers close to the
\code{tree.root}, since a case can only be classified to sibling classes of
those selected using the majority rule in classes closer to the
\code{tree.root}.\cr If the blue values in the figure below are the
proportion of votes for a given case in a given local classifier, then the
case would be classified to class \emph{A} in local classifier \emph{C.1},
and then to class \emph{A.2} in local classifier \emph{C.2}.\cr

\strong{The \code{"multiplicative.majority"} method:}  \cr First multiply the
proportion of votes along each path from the \code{tree.root} to any of the
terminal nodes (see: \code{\link{GetMultPropVotes}}). The result is a vector
of probabilities for each terminal node, similar to that returned from flat
classification. Next, apply the majority rule and select the terminal node
that received the highest multiplicative proportion of votes. This method
give less emphasis to local classifiers near the root of the class hierarchy,
but may depend on the number of siblings nodes. \cr If the red values in the
figure below are the multiplication of votes along each path to all 4
terminal nodes for a given case, then the case would be classified to class
\emph{B.2} \cr

\strong{The \code{"multiplicative.permutation"} method: }\cr Similar to
\code{multiplicative.permutation}, but instead of applying a majority rule,
the method randomly select a terminal node based on the multiplicative
probabilities. The user defines the number of permutations and accuracy is
assessed separately for each permutation. When most cases are classified to
the same class in various classification trees (i.e., when the proportion of
votes for one category is close to 1), the mean accuracy over all permutation
should be similar to that achieved under the \code{"multiplicative.majority"}
method.\cr In the example in the figure below, each permutation will choose a
class as random draw with probabilities equal to the values in red. \cr

  \if{html}{\figure{StepMultExp.jpeg}}
  \if{latex}{\figure{StepMultExp.jpeg}{options: width=7cm}}

 \strong{The two options of \code{per.index}:} \cr
   {\tabular{lll}{
   \code{"flat.measures"} \tab \tab Accuracy measures that do not consider
   the hierarchical structure of the classes. In this case, the accuracy
   measures returned are those returned by \code{\link{confusionMatrix}} of
   the \code{caret} package. \cr \cr \cr

   \code{"hie.F.measure"} \tab \tab Accuracy measures that account for the
   hierarchical structure of the classes. See details in
   \code{\link{HieFMeasure}} and reference within. \cr}}
}
\section{The hie.performance data frame}{


The \code{hie.performance} data frame contains the following overall
performance measures:

 {\tabular{lll}{
   \code{"crisp.rule"}     \tab \tab The crisp rule used to create the crisp
   classification. \cr

   \code{"Accuracy"}       \tab \tab The overall accuracy. \cr

   \code{"Kappa"}          \tab \tab The unweighted Kappa statistic. \cr

   \code{"AccuracyLower"}  \tab \tab The lower bound of the 95 confidence
   interval around the overall error based on \code{\link{binom.test}}. \cr

   \code{"AccuracyUpper"}  \tab \tab The upper bound of the 95 confidence
   interval. \cr

   \code{"AccuracyNull"}   \tab \tab The expected null accuracy. \cr

   \code{"AccuracyPValue"} \tab \tab One side test to see if accuracy is
   better than "no information rate". \cr

   \code{"AccuracyNull"}   \tab \tab A p-value from McNemar's test using
   \code{\link{mcnemar.test}} (may return \code{NA}). \cr

   \code{"h.precision"}    \tab \tab The hierarchical precision from
   \code{\link{HieFMeasure}}. \cr

   \code{"h.recall"}       \tab \tab The hierarchical recall from
   \code{\link{HieFMeasure}}. \cr

   \code{"h.F.measure"}    \tab \tab The hierarchical F measure from
   \code{\link{HieFMeasure}}. \cr }}
}
\examples{
# create a random HRF dataset and run HRF analysis
set.seed(354)
random.hRF    <- RandomHRF(num.term.nodes = 20, tree.depth = 4)
train.data    <- random.hRF$train.data
hie.RF.random <- RunHRF(train.data = train.data,
                        case.ID    = "case.ID",
                        hie.levels = c(2:(random.hRF$call$tree.depth + 1)))

# assess performance
perf.hRF.random <- PerformanceHRF(hie.RF     = hie.RF.random,
                                  perm.num   = 20,
                                  div.print  = 5)

### Extract values ###
names(perf.hRF.random)
raw.vote.random               <- perf.hRF.random$raw.vote
crisp.case.class.random       <- perf.hRF.random$crisp.case.class
hie.performance.random        <- perf.hRF.random$hie.performance
multiplicative.prop.random    <- perf.hRF.random$multiplicative.prop
nodes.measures.columns.random <- perf.hRF.random$nodes.measures.columns
hie.perf.call.random          <- perf.hRF.random$call

#### example with the OliveOilHie dataset
data(OliveOilHie)
hie.RF.OO <- RunHRF(train.data        = OliveOilHie,
                    case.ID           = "case.ID",
                    hie.levels        = c(2:4),
                    mtry              = "tuneRF2",
                    internal.end.path = TRUE)

perf.hRF.olive <- PerformanceHRF(hie.RF    = hie.RF.OO,
                                 crisp.rule  = c("multiplicative.majority"))

### Extract values ###
crisp.case.class.olive <- perf.hRF.olive$crisp.case.class
hie.performance.olive <- perf.hRF.olive$hie.performance

### plotting option ###
# create a confusion matrix
conf.matr.olive <- as.data.frame(table(crisp.case.class.olive$obs.term.node,
                      crisp.case.class.olive$multiplicative.majority.rule))
# use the PlotImportanceHie to plot the confusion matrix
PlotImportanceHie(input.data = conf.matr.olive,
                  X.data     = 1,
                  Y.data     = 2,
                  imp.data   = 3,
                  plot.type  = "Tile",
                  X.Title    = c("Observed"),
                  Y.Title    = c("mMultiplicative majority rule"),
                  imp.title  = c("frequency"),
                  low.col    = "darkslategray4",
                  high.col   = "red",
                  geom.tile.bor.col = "gray20")
}
\author{
Yoni Gavish <gavishyoni@gmail.com>
}
\seealso{
\code{\link{RunHRF}} for running a hierarchical randomForest analysis,
\code{\link{PerformanceNewHRF}} for performance analysis on \code{new.data}
with observed terminal node, \code{\link{PerformanceFlatRF}} for hierarchical
and flat performance analyses for a flat randomForest,
\code{\link{HieFMeasure}} for additional information on the hierarchical
performance measures.
}

